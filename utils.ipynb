{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "from random import shuffle\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(sent):\n",
    "\n",
    "    sent=re.sub(r'[.,\\(\\)?!;]',\"\",sent.lower())\n",
    "    sent = sent.replace('/',' ')\n",
    "    sent = tokenizer.tokenize(sent)\n",
    "    sent=' '.join( [w for w in sent if len(w)>1] )\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePaddedList(sent_contents, maxl, pad_symbol= '<pad>'):\n",
    "    T = []\n",
    "    for sent in sent_contents:\n",
    "        t = []\n",
    "        lenth = len(sent)\n",
    "        for i in range(lenth):\n",
    "            t.append(sent[i])\n",
    "        for i in range(lenth,maxl):\n",
    "            t.append(pad_symbol)\n",
    "        T.append(t)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDistanceList(lista):\n",
    "    sent_list = sum(lista, [])\n",
    "    wf = {}\n",
    "    for sent in sent_list:\n",
    "        for w in sent:\n",
    "            if w in wf:\n",
    "                wf[w] += 1\n",
    "            else:\n",
    "                wf[w] = 0\n",
    "\n",
    "    wl = []\n",
    "    i = 1\n",
    "    for w,f in wf.items():\n",
    "        wl.append(w)\n",
    "    return wl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWordToId(sent_contents, word2id):\n",
    "    T = []\n",
    "    for sent in sent_contents:\n",
    "        t = []\n",
    "        for w in sent:\n",
    "            #t.append(word_list.index(w))\n",
    "            t.append(word2id[w])\n",
    "        T.append(t)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWordToId_list(sent_contents, word_list):\n",
    "    T = []\n",
    "    for sent in sent_contents:\n",
    "        t = []\n",
    "        for w in sent:\n",
    "            t.append(word_list.index(w))            \n",
    "        T.append(t)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabelToId_snp_positive(sent_lables, label_dict):\n",
    "    return [label_dict[label] for label in sent_lables]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabelToId_snp(sent_lables, label_dict):\n",
    "    return [int (label == 'positive') for label in sent_lables]\n",
    "    #return [label_dict[label] for label in sent_lables]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabelToId_befree_EUADR(sent_lables, label_dict):\n",
    "    return [int (label != 'FA') for label in sent_lables]\n",
    "        # SA,NA,PA ==>1    FA ==>0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabelToId_befree(sent_lables, label_dict):\n",
    "    if len(label_dict) > 2:\n",
    "        lables=[]\n",
    "        for label in sent_lables:\n",
    "            try:\n",
    "                lables.append(label_dict[label])  \n",
    "            except:  # only for one sample which  is P\n",
    "                lables.append(1)\n",
    "                \n",
    "        return lables\n",
    "                \n",
    "    else:\n",
    "        return [int (label != 'F') for label in sent_lables]\n",
    "        # Y,N ==>1    F ==>0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabelToId(sent_lables, label_dict):\n",
    "    if len(label_dict) > 2:\n",
    "        return [label_dict[label] for label in sent_lables]\n",
    "    else:\n",
    "        return [int (label != 'Negative') for label in sent_lables]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_positive_negative(pos_file,neg_file,neg_ratio,num_pos_samples,random_seed=1337):\n",
    "    num_neg_samples=neg_ratio*num_pos_samples\n",
    "    pos_samples= pd.read_csv(pos_file, encoding='latin-1')\n",
    "    pos_samples=pos_samples[pos_samples['gene_mention'].str.lower()!=pos_samples['disease_mention'].str.lower()]\n",
    "    pos_samples=pos_samples.sample(num_pos_samples,random_state=random_seed)\n",
    "    neg_samples= pd.read_csv(neg_file, encoding='latin-1')\n",
    "    neg_samples=neg_samples[neg_samples['gene_mention'].str.lower()!=neg_samples['disease_mention'].str.lower()]\n",
    "    neg_samples=neg_samples.sample(num_neg_samples,random_state=random_seed)\n",
    "    return pos_samples,neg_samples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRead_snp(fname, ignore_neutral=False,keep_only_positive=False,read_confidence=False):\n",
    "    print(\"Input File Reading\")\n",
    "    samples= pd.read_csv(fname,sep='\\t')\n",
    "    sent_lengths   = []\n",
    "    sent_contents  = []\n",
    "    sent_lables    = []\n",
    "    sent_confidences = []\n",
    "    entity1_list   = []\n",
    "    entity2_list   = []\n",
    "    gene_id_list=[]\n",
    "    disease_id_list=[]\n",
    "    gene_symbol_list=[]\n",
    "    confidence=0\n",
    "    \n",
    "    for i,sample in samples.iterrows():\n",
    "        e1=sample['snp']\n",
    "        e1_t='snp'\n",
    "        e2=sample['phenotype']\n",
    "        e2_t='phenotype'\n",
    "        relation=sample['lable']\n",
    "        if read_confidence:\n",
    "            confidence=sample['confidence']\n",
    "        \n",
    "        if keep_only_positive:\n",
    "            if relation!='positive':\n",
    "                continue\n",
    "            \n",
    "        if ignore_neutral:\n",
    "            if relation=='neutral':\n",
    "                continue\n",
    "            \n",
    "        sent=sample['sentence']\n",
    "        sent=sent.lower()\n",
    "        e1=e1.lower()\n",
    "        e2=e2.lower()\n",
    "        \n",
    "        \n",
    "        snp_start=int(sample['snp_start'])\n",
    "        snp_end=int(sample['snp_end'])\n",
    "        pheno_start=int(sample['pheno_start'])\n",
    "        pheno_end=int(sample['pheno_end'])\n",
    "        \n",
    "        if snp_start > pheno_start:\n",
    "            sent=sent[:snp_start]+ ' FIRST_ENTITY ' + sent[snp_end:]\n",
    "            sent=sent[:pheno_start]+ ' SECOND_ENTITY ' + sent[pheno_end:]\n",
    "        else:\n",
    "            sent=sent[:pheno_start]+ ' SECOND_ENTITY ' + sent[pheno_end:]\n",
    "            sent=sent[:snp_start]+ ' FIRST_ENTITY ' + sent[snp_end:]\n",
    "        sent=preProcess(sent)\n",
    "        sent_contents.append(sent)\n",
    "      \n",
    "     \n",
    "        try:\n",
    "            sent_splitted = sent.split()       \n",
    "            s1 = sent_splitted.index('first_entity')\n",
    "            s2 = sent_splitted.index('second_entity') \n",
    "        except:\n",
    "            print('input file contains illegal format ')\n",
    "            break\n",
    "        \n",
    "        \n",
    "        entity1_list.append([e1, e1_t])\n",
    "        entity2_list.append([e2, e2_t])\n",
    "        sent_lables.append(relation)\n",
    "        sent_confidences.append(confidence)\n",
    "        \n",
    "        \n",
    "    return sent_contents, entity1_list, entity2_list, sent_lables,sent_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRead_befree(fname):\n",
    "    print(\"Input File Reading\")\n",
    "    \n",
    "    samples= pd.read_csv(fname)\n",
    "    sent_lengths   = []\n",
    "    sent_contents  = []\n",
    "    sent_lables    = []\n",
    "    entity1_list   = []\n",
    "    entity2_list   = []\n",
    "    gene_id_list=[]\n",
    "    disease_id_list=[]\n",
    "    gene_symbol_list=[]\n",
    "\n",
    "    \n",
    "    for i,sample in samples.iterrows():\n",
    "        e1=sample['gene_mention']\n",
    "        e1_t='gene'\n",
    "        e2=sample['disease_mention']\n",
    "        e2_t='disease'\n",
    "        relation=sample['associationType']\n",
    "        sent=sample['raw_sentence']\n",
    "        sent=sent.lower()\n",
    "        e1=e1.lower()\n",
    "        e2=e2.lower()\n",
    "        \n",
    "        \n",
    "        offset_gene=sample['GENE_ENTITY_OFFSET'].split('#')\n",
    "        offset_gene=[int (i) for i in offset_gene]\n",
    "        \n",
    "        offset_disease=sample['DISEASE_ENTITY_OFFSET'].split('#')\n",
    "        offset_disease=[int (i) for i in offset_disease]\n",
    "        \n",
    "        if offset_gene[0] > offset_disease[0]:\n",
    "            sent=sent[:offset_gene[0]]+ ' FIRST_ENTITY ' + sent[offset_gene[1]:]\n",
    "            sent=sent[:offset_disease[0]]+ ' SECOND_ENTITY ' + sent[offset_disease[1]:]\n",
    "        else:\n",
    "            sent=sent[:offset_disease[0]]+ ' SECOND_ENTITY ' + sent[offset_disease[1]:]\n",
    "            sent=sent[:offset_gene[0]]+ ' FIRST_ENTITY ' + sent[offset_gene[1]:]\n",
    "        \n",
    "        sent=preProcess(sent)\n",
    "        sent_contents.append(sent)\n",
    "      \n",
    "     \n",
    "        try:\n",
    "            sent_splitted = sent.split()       \n",
    "            s1 = sent_splitted.index('first_entity')\n",
    "            s2 = sent_splitted.index('second_entity') \n",
    "        except:\n",
    "            print('input file contains illegal format ')\n",
    "            break\n",
    "        \n",
    "        \n",
    "        entity1_list.append([e1, e1_t])\n",
    "        entity2_list.append([e2, e2_t])\n",
    "        sent_lables.append(relation)\n",
    "        gene_id_list.append(sample['geneId'])\n",
    "        disease_id_list.append(sample['diseaseId'])\n",
    "        gene_symbol_list.append(sample['geneSymbol'])\n",
    "        \n",
    "        \n",
    "    return sent_contents, entity1_list, entity2_list, sent_lables, gene_id_list,disease_id_list,gene_symbol_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRead_befree_EUADR(fname):\n",
    "    print(\"Input File Reading\")\n",
    "    samples=pd.read_csv(fname,sep='\\t',encoding='latin',keep_default_na=False)\n",
    "    sent_lengths   = []\n",
    "    sent_contents  = []\n",
    "    sent_lables    = []\n",
    "    entity1_list   = []\n",
    "    entity2_list   = []\n",
    "\n",
    "    for i,sample in samples.iterrows():\n",
    "        e1=sample['ENTITY1_TEXT']\n",
    "        e1_t='gene'\n",
    "        e2=sample['ENTITY2_TEXT']\n",
    "        e2_t='disease'\n",
    "        relation=sample['ASSOCIATION_TYPE']\n",
    "        sent=sample['SENTENCE']\n",
    "        sent=sent.lower()\n",
    "        e1=e1.lower()\n",
    "        e2=e2.lower()\n",
    "        \n",
    "        \n",
    "        offset_gene_start=int(sample['ENTITY1_INI'])\n",
    "        offset_gene_end=int(sample['ENTITY1_END'])\n",
    "        \n",
    "        \n",
    "        offset_disease_start=int(sample['ENTITY2_INI'])\n",
    "        offset_disease_end=int(sample['ENTITY2_END'])\n",
    "        \n",
    "        \n",
    "        if offset_gene_start > offset_disease_start:\n",
    "            sent=sent[:offset_gene_start]+ ' FIRST_ENTITY ' + sent[offset_gene_end:]\n",
    "            sent=sent[:offset_disease_start]+ ' SECOND_ENTITY ' + sent[offset_disease_end:]\n",
    "        else:            \n",
    "            sent=sent[:offset_disease_start]+ ' SECOND_ENTITY ' + sent[offset_disease_end:]\n",
    "            sent=sent[:offset_gene_start]+ ' FIRST_ENTITY ' + sent[offset_gene_end:]\n",
    "        \n",
    "        sent=preProcess(sent)\n",
    "        sent_contents.append(sent)\n",
    "      \n",
    "     \n",
    "        try:\n",
    "            sent_splitted = sent.split()       \n",
    "            s1 = sent_splitted.index('first_entity')\n",
    "            s2 = sent_splitted.index('second_entity') \n",
    "        except:\n",
    "            print('input file contains illegal format ')\n",
    "            break\n",
    "        \n",
    "        \n",
    "        entity1_list.append([e1, e1_t])\n",
    "        entity2_list.append([e2, e2_t])\n",
    "        sent_lables.append(relation)\n",
    "        \n",
    "    return sent_contents, entity1_list, entity2_list, sent_lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRead(fname,max_length=2000):\n",
    "    print(\"Input File Reading\")\n",
    "    \n",
    "    samples= pd.read_csv(fname, encoding='latin-1')\n",
    "    sent_lengths   = []\n",
    "    sent_contents  = []\n",
    "    sent_lables    = []\n",
    "    entity1_list   = []\n",
    "    entity2_list   = []\n",
    "    gene_id_list=[]\n",
    "    disease_id_list=[]\n",
    "    for i,sample in samples.iterrows():\n",
    "        \n",
    "        e1=sample['gene_mention']\n",
    "        e1_t='gene'\n",
    "        e2=sample['disease_mention']\n",
    "        e2_t='disease'\n",
    "        relation=sample['associationType']\n",
    "\n",
    "        sent=sample['raw_sentence']\n",
    "        sent=sent.lower()\n",
    "        e1=e1.lower()\n",
    "        e2=e2.lower()\n",
    "        # if e1 contains e2 replcae the e1 first\n",
    "        if e1.find(e2)!=-1:\n",
    "            sent = sent.replace(e1, ' FIRST_ENTITY ')\n",
    "            sent = sent.replace(e2, ' SECOND_ENTITY ')\n",
    "        else:\n",
    "            sent = sent.replace(e2, ' SECOND_ENTITY ')\n",
    "            sent = sent.replace(e1, ' FIRST_ENTITY ')\n",
    "        sent=preProcess(sent)\n",
    "        try:\n",
    "            sent_splitted = sent.split()     \n",
    "            if len(sent_splitted) > 100:\n",
    "                continue\n",
    "            s1 = sent_splitted.index('first_entity')\n",
    "            s2 = sent_splitted.index('second_entity') \n",
    "        except:\n",
    "            print('input file contains illegal format ')            \n",
    "            break\n",
    "\n",
    "        sent_contents.append(sent)\n",
    "        entity1_list.append([e1, e1_t])\n",
    "        entity2_list.append([e2, e2_t])\n",
    "        sent_lables.append(relation)\n",
    "        gene_id_list.append(sample['geneId'])\n",
    "        disease_id_list.append(sample['diseaseId'])\n",
    "        \n",
    "    return sent_contents, entity1_list, entity2_list, sent_lables, gene_id_list,disease_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordList_and_distances_snp(sent_list):\n",
    "    \n",
    "    word_list = []\n",
    "    d1_list = []\n",
    "    d2_list = []\n",
    "    \n",
    "    i=0\n",
    "    for sent in sent_list:\n",
    "        i=i+1\n",
    "        sent_list1 = sent.split()       \n",
    "        try:\n",
    "            s1 = sent_list1.index('first_entity')\n",
    "            s2 = sent_list1.index('second_entity') \n",
    "        except:\n",
    "            print(sent)\n",
    "            break\n",
    "            \n",
    "        \n",
    "        # distance1 feature\t\n",
    "        d1 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s1 :\n",
    "                d1.append(str(i - s1))\n",
    "            elif i > s1 :\n",
    "                d1.append(str(i - s1 ))\n",
    "            else:\n",
    "                d1.append('0')\n",
    "\n",
    "        #distance2 feature\n",
    "\n",
    "        d2 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s2:\n",
    "                d2.append(str(i - s2))\n",
    "            elif i > s2:\n",
    "                d2.append(str(i - s2))\n",
    "            else:\n",
    "                d2.append('0')\n",
    "    \n",
    "        word_list.append(sent_list1)\n",
    "        d1_list.append(d1)\n",
    "        d2_list.append(d2)\n",
    "    \n",
    "        \n",
    "    return word_list, d1_list, d2_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordList_and_distances_befree(sent_list):\n",
    "    word_list = []\n",
    "    distance1_list = []\n",
    "    distance2_list = []\n",
    "    type_list = []\n",
    "    i=0\n",
    "    for sent in sent_list:\n",
    "        i=i+1\n",
    "        sent_list1 = sent.split()       \n",
    "        try:\n",
    "            s1 = sent_list1.index('first_entity')\n",
    "            s2 = sent_list1.index('second_entity') \n",
    "        except:\n",
    "            print(sent)\n",
    "            break\n",
    "            \n",
    "        \n",
    "        # distance1 feature\n",
    "        d1 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s1 :\n",
    "                d1.append(str(i - s1))\n",
    "            elif i > s1 :\n",
    "                d1.append(str(i - s1 ))\n",
    "            else:\n",
    "                d1.append('0')\n",
    "\n",
    "        #distance2 feature\n",
    "\n",
    "        d2 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s2:\n",
    "                d2.append(str(i - s2))\n",
    "            elif i > s2:\n",
    "                d2.append(str(i - s2))\n",
    "            else:\n",
    "                d2.append('0')\n",
    "        \n",
    "        \n",
    "        word_list.append(sent_list1)\n",
    "        distance1_list.append(d1)\n",
    "        distance2_list.append(d2)\n",
    "        \n",
    "        \n",
    "    return word_list, distance1_list, distance2_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordList_and_distances_Corpus(sent_list, entity1_list, entity2_list):\n",
    "    \n",
    "    word_list = []\n",
    "    d1_list = []\n",
    "    d2_list = []\n",
    "    \n",
    "    i=0\n",
    "    for sent, ent1, ent2 in zip(sent_list, entity1_list, entity2_list):\n",
    "        i=i+1\n",
    "        sent_list1 = sent.split()       \n",
    "        try:\n",
    "            s1 = sent_list1.index('first_entity')\n",
    "            s2 = sent_list1.index('second_entity') \n",
    "        except:\n",
    "            print(sent)\n",
    "            break\n",
    "            \n",
    "        \n",
    "        # distance1 feature\t\n",
    "        d1 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s1 :\n",
    "                d1.append(str(i - s1))\n",
    "            elif i > s1 :\n",
    "                d1.append(str(i - s1 ))\n",
    "            else:\n",
    "                d1.append('0')\n",
    "\n",
    "        #distance2 feature\n",
    "\n",
    "        d2 = []\n",
    "        for i in range(len(sent_list1)):\n",
    "            if i < s2:\n",
    "                d2.append(str(i - s2))\n",
    "            elif i > s2:\n",
    "                d2.append(str(i - s2))\n",
    "            else:\n",
    "                d2.append('0')\n",
    "        \n",
    "\n",
    "        word_list.append(sent_list1)\n",
    "        d1_list.append(d1)\n",
    "        d2_list.append(d2)\n",
    "        \n",
    "        \n",
    "    return word_list, d1_list, d2_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(samples,train_frac,test_frac,val_frac,random_seed=1337):\n",
    "    \n",
    "    val_param=1-val_frac\n",
    "    train, test, val = np.split(samples.sample(frac=1,random_state=random_seed), [int(train_frac*len(samples)), int(val_param*len(samples))])\n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Function for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordList(lista):\n",
    "    sent_list = sum(lista, [])\n",
    "    wf = {}\n",
    "    for sent in sent_list:\n",
    "        for w in sent:\n",
    "            if w in wf:\n",
    "                wf[w] += 1\n",
    "            else:\n",
    "                wf[w] = 0\n",
    "\n",
    "    wl = []\n",
    "    i = 1\n",
    "\n",
    "    wl.append('<pad>')\n",
    "    wl.append('<unkown>')\n",
    "    for w,f in wf.items():\n",
    "        wl.append(w)\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mapping(Tr_word_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    dico = create_dico(Tr_word_list)\n",
    "    dico['<unknown>'] = 10000000\n",
    "    dico['<pad>'] = 10000000 + 1\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print (\"Found %i unique words (%i in total)\" % (len(dico), sum(len(x) for x in Tr_word_list)))\n",
    "    return dico, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWordEmb_fastText(dico_words,id_to_word,word_to_id, fname, embSize=300):\n",
    "    import io\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    c_found = 0\n",
    "    print (\"Reading word vectors\")\n",
    "    word_emb_weight = np.zeros((len(dico_words), embSize))\n",
    "    foundWords={}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in dico_words:\n",
    "            word_emb_weight[word_to_id[tokens[0]]] = list(map(float, tokens[1:]))\n",
    "            foundWords[tokens[0]]=1  # 1 is not used\n",
    "            c_found += 1\n",
    "        \n",
    "    n_words = len(dico_words)    \n",
    "    count=0\n",
    "    for i in range(n_words):\n",
    "        word = id_to_word[i]\n",
    "        if word not in foundWords:\n",
    "            count += 1\n",
    "            word_emb_weight[i]=np.random.rand(embSize)\n",
    "    \n",
    "    print (\"number of unknown word in word embedding\", count)\n",
    "    print (\"number of known word in word embedding\", c_found)\n",
    "    \n",
    "    return word_emb_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWordEmb(dico_words,id_to_word,word_to_id, fname, embSize=200,limit=None):\n",
    "    from gensim.models.keyedvectors import KeyedVectors   \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=True,limit=limit)\n",
    "    print (\"Reading word vectors\")\n",
    "    word_emb_weight = np.zeros((len(dico_words), embSize))\n",
    "    c_found = 0\n",
    "    n_words = len(dico_words)\n",
    "    count=0\n",
    "    for i in range(n_words):\n",
    "        word = id_to_word[i]\n",
    "        if word in model:\n",
    "            word_emb_weight[i] = model[word]\n",
    "            c_found += 1\n",
    "        else:\n",
    "            count += 1\n",
    "            word_emb_weight[i]=np.random.rand(embSize)\n",
    "    \n",
    "    print ('Loaded %i pretrained embeddings.' % len(model.vocab))   \n",
    "    print (\"number of unknown word in word embedding\", count)\n",
    "    \n",
    "    return word_emb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLongestSent(Tr_word_list, Te_word_list):\n",
    "    combine_list = Tr_word_list + Te_word_list\n",
    "    a = max([len(sent) for sent in combine_list])\n",
    "    return a\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSentLengths(tr_te_list):\n",
    "    lis = []\n",
    "    for lists in tr_te_list:\n",
    "        lis.append([len(l) for l in lists])\n",
    "    return lis\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddData(listL, maxl,padd_num=0): #W_batch, d1_tatch, d2_batch, t_batch)\n",
    "    rlist = []\n",
    "    for mat in listL:\n",
    "        mat_n = []\n",
    "        for row in mat:\n",
    "            lenth = len(row)\n",
    "            t = []\n",
    "            for i in range(lenth):\n",
    "                t.append(row[i])\n",
    "            for i in range(lenth, maxl):\n",
    "                t.append(padd_num)\n",
    "            mat_n.append(t)\n",
    "        rlist.append(np.array(mat_n)) \n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddData(listL, maxl,padd_num=0): #W_batch, d1_tatch, d2_batch, t_batch)\n",
    "    rlist = []\n",
    "    for mat in listL:\n",
    "        mat_n = []\n",
    "        for row in mat:\n",
    "            lenth = len(row)\n",
    "            t = []\n",
    "            if (lenth>maxl):\n",
    "                lenth=maxl\n",
    "            for i in range(lenth):\n",
    "                t.append(row[i])\n",
    "            for i in range(lenth, maxl):\n",
    "                t.append(padd_num)\n",
    "            mat_n.append(t)\n",
    "        rlist.append(np.array(mat_n)) \n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBalence(Tr_sent_contents, Tr_entity1_list, Tr_entity2_list, Tr_sent_lables):\n",
    "    sent_contents=[]; entity1_list=[]; entity2_list=[]; sent_lables=[];\n",
    "    other = []\n",
    "    clas = []\n",
    "    for sent,e1,e2,lab in zip(Tr_sent_contents, Tr_entity1_list, Tr_entity2_list, Tr_sent_lables):\n",
    "        if lab == 'false' :\n",
    "            other.append([sent, e1, e2, lab])\n",
    "        else:\n",
    "            clas.append([sent, e1, e2, lab])\n",
    "\n",
    "    random.shuffle(other)\n",
    "\n",
    "    neg = other[0 : 3*len(clas)]\n",
    "    l = neg+clas\n",
    "    for sent,e1,e2,lab in l:\n",
    "        sent_contents.append(sent)\n",
    "        entity1_list.append(e1)\n",
    "        entity2_list.append(e2)\n",
    "        sent_lables.append(lab)\n",
    "    return sent_contents, entity1_list, entity2_list, sent_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_split(data, percent_validation, percent_test, seed=1337):\n",
    "\n",
    "    \n",
    "    random.seed(seed)\n",
    "    shuffle(data)\n",
    "    nrows = len(data)\n",
    "    test_len = int(percent_test/100.0 * nrows)\n",
    "    val_len = int(percent_validation/100.0 * nrows)\n",
    "    train_len=nrows -(val_len+test_len)\n",
    "   \n",
    "    return data[:train_len], data[train_len:train_len+val_len], data[train_len+val_len:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(Snt, Gen=None, Dis=None):\n",
    "    \"\"\"\n",
    "    Get a sentence,gene and disease mentions and returns frequency of words in the path connecting them\n",
    "    \n",
    "    Reuirements:\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    sudo pip3 install networkx\n",
    "    sudo pip3 install spacy\n",
    "    sudo -H pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz\n",
    "    sudo python3 -m spacy download en_core_web_lg\n",
    "    \n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for token in Snt:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "    graph = nx.Graph(edges)\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=Gen, target=Dis)\n",
    "    except nx.NetworkXNoPath:\n",
    "        path = []\n",
    "    #filtered_words = [word for word in path if word not in stop]\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_common_anssestor(Snt, Gen=None, Dis=None):\n",
    "    \"\"\"\n",
    "    Get a sentence,gene and disease mentions and returns the root word  of words (lowest_common_anssestor) in the path connecting them\n",
    "    \n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    edges = []\n",
    "    nodes=[]\n",
    "    for token in Snt:\n",
    "        nodes.append('{0}'.format(token))\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "    #graph = nx.Graph(edges)\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    common=nx.algorithms.lowest_common_ancestor(G,Gen,Dis)\n",
    "    return mylem.lemmatize(common, pos='v')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(Snt, Gen=None, Dis=None):\n",
    "    \"\"\"\n",
    "    Get a sentence,gene and disease mentions and returns frequency of words in the path connecting them\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    edges = []\n",
    "    for token in Snt:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "    graph = nx.Graph(edges)\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=Gen, target=Dis)\n",
    "        return  nx.ancestors(graph,Dis)\n",
    "        \n",
    "    except nx.NetworkXNoPath:\n",
    "        path = []\n",
    "    #removing stop words\n",
    "    filtered_words = [word for word in path if word not in stop]\n",
    "\n",
    "    #Frequency of words\n",
    "    counts = Counter(filtered_words)\n",
    "\n",
    "    \n",
    "    return counts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
